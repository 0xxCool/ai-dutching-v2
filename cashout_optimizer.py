"""
üí∞ CASHOUT OPTIMIZER

Optimiert Cashout-Entscheidungen durch:
1. Heuristische Regeln (Rule-based)
2. Expected Value Berechnung
3. Deep Q-Learning (Advanced)

ROI-Steigerung: +15-25% durch optimale Cashout-Timings
"""

import numpy as np
import pandas as pd
from typing import Dict, Tuple, Optional, List
from dataclasses import dataclass
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Optional: Deep Learning
try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False


# ==========================================================
# CONFIGURATION
# ==========================================================
@dataclass
class CashoutConfig:
    """Konfiguration f√ºr Cashout-Optimizer"""
    # Thresholds
    min_profit_threshold: float = 1.20  # Min 20% Profit
    secure_profit_ratio: float = 0.80  # Sichere 80% des EV
    trailing_stop_pct: float = 0.10  # Stop bei 10% vom Peak

    # Risk Management
    max_hold_time: int = 90  # Maximum Minuten zu halten
    confidence_threshold: float = 0.60  # Min Confidence f√ºr Hold

    # EV Adjustments
    bookmaker_margin: float = 0.05  # 5% Bookmaker Margin
    variance_factor: float = 1.2  # Variance Adjustment


# ==========================================================
# STATE REPRESENTATION
# ==========================================================
@dataclass
class BetState:
    """Aktueller State einer laufenden Wette"""
    # Original Bet
    original_stake: float
    original_odds: float
    selection: str  # 'Home', 'Draw', 'Away'

    # Current Match State
    current_time: int  # Minute (0-90+)
    home_score: int
    away_score: int

    # Live Data
    current_home_win_prob: float
    current_draw_prob: float
    current_away_win_prob: float

    # Cashout Offer
    cashout_offer: float

    # Historical
    peak_cashout: float = 0.0
    min_cashout: float = float('inf')

    # xG Data (optional)
    home_xg: float = 0.0
    away_xg: float = 0.0
    home_xg_live: float = 0.0
    away_xg_live: float = 0.0


# ==========================================================
# HEURISTIC CASHOUT OPTIMIZER
# ==========================================================
class HeuristicCashoutOptimizer:
    """
    Rule-based Cashout Optimizer

    Verwendet empirische Regeln f√ºr Cashout-Entscheidungen
    """

    def __init__(self, config: CashoutConfig = None):
        self.config = config or CashoutConfig()

    def calculate_expected_value(self, state: BetState) -> float:
        """
        Berechne Expected Value von "Hold" (nicht cashout)

        Returns:
            EV in Euro
        """
        potential_payout = state.original_stake * state.original_odds

        # Bestimme Gewinnwahrscheinlichkeit
        if state.selection == 'Home':
            win_prob = state.current_home_win_prob
        elif state.selection == 'Draw':
            win_prob = state.current_draw_prob
        else:  # Away
            win_prob = state.current_away_win_prob

        # EV = P(Win) * Payout + P(Loss) * 0
        ev = win_prob * potential_payout

        return ev

    def calculate_confidence(self, state: BetState) -> float:
        """
        Berechne Confidence Score (0-1)

        Faktoren:
        - Verbleibende Zeit
        - xG-Momentum
        - Score-Differenz
        """
        confidence = 0.5  # Baseline

        # Faktor 1: Verbleibende Zeit (weniger Zeit = h√∂here Sicherheit)
        time_remaining = max(0, 90 - state.current_time)
        time_factor = 1 - (time_remaining / 90)
        confidence += time_factor * 0.3

        # Faktor 2: Score-Vorteil
        score_diff = state.home_score - state.away_score

        if state.selection == 'Home' and score_diff > 0:
            confidence += min(score_diff * 0.15, 0.3)
        elif state.selection == 'Away' and score_diff < 0:
            confidence += min(abs(score_diff) * 0.15, 0.3)

        # Faktor 3: F√ºhrung verteidigen vs aufholen
        if state.current_time > 70:  # Sp√§tes Spiel
            if (state.selection == 'Home' and score_diff > 0) or \
               (state.selection == 'Away' and score_diff < 0):
                confidence += 0.2  # F√ºhrung verteidigen ist einfacher

        # Clip
        confidence = np.clip(confidence, 0, 1)

        return confidence

    def should_cashout(self, state: BetState) -> Tuple[bool, str, float]:
        """
        Entscheide ob Cashout

        Returns:
            (should_cashout, reason, recommended_amount)
        """
        # Update peak
        if state.cashout_offer > state.peak_cashout:
            state.peak_cashout = state.cashout_offer
        if state.cashout_offer < state.min_cashout:
            state.min_cashout = state.cashout_offer

        # Berechne Metriken
        ev_hold = self.calculate_expected_value(state)
        ev_cashout = state.cashout_offer
        confidence = self.calculate_confidence(state)

        potential_payout = state.original_stake * state.original_odds

        # REGEL 1: Sichere Profit (Cashout >= 80% vom EV)
        if ev_cashout >= ev_hold * self.config.secure_profit_ratio:
            if ev_cashout > state.original_stake * self.config.min_profit_threshold:
                return (
                    True,
                    f"Secure Profit: ‚Ç¨{ev_cashout:.2f} ‚â• {self.config.secure_profit_ratio*100:.0f}% of EV",
                    ev_cashout
                )

        # REGEL 2: Trailing Stop (Cashout f√§llt von Peak)
        cashout_drop = (state.peak_cashout - state.cashout_offer) / state.peak_cashout
        if cashout_drop > self.config.trailing_stop_pct:
            if state.cashout_offer > state.original_stake:  # Nur wenn Profit
                return (
                    True,
                    f"Trailing Stop: Drop {cashout_drop*100:.1f}% from peak ‚Ç¨{state.peak_cashout:.2f}",
                    state.cashout_offer
                )

        # REGEL 3: Low Confidence + Profit available
        if confidence < self.config.confidence_threshold:
            if state.cashout_offer > state.original_stake * 1.15:  # Min 15% Profit
                return (
                    True,
                    f"Low Confidence ({confidence:.2%}) + Profit Available",
                    state.cashout_offer
                )

        # REGEL 4: Late Game + Verlieren
        if state.current_time > 80:
            losing = False

            if state.selection == 'Home' and state.home_score < state.away_score:
                losing = True
            elif state.selection == 'Away' and state.away_score < state.home_score:
                losing = True
            elif state.selection == 'Draw' and state.home_score != state.away_score:
                losing = True

            if losing:
                # Salvage was m√∂glich ist
                if state.cashout_offer > state.original_stake * 0.30:  # Min 30% zur√ºck
                    return (
                        True,
                        f"Late Game ({state.current_time}') + Losing Position - Salvage Loss",
                        state.cashout_offer
                    )

        # REGEL 5: Maximum Hold Time √ºberschritten
        if state.current_time > self.config.max_hold_time:
            if state.cashout_offer > state.original_stake:
                return (
                    True,
                    f"Max Hold Time ({self.config.max_hold_time} min) exceeded",
                    state.cashout_offer
                )

        # REGEL 6: Partial Cashout bei hohem Profit
        profit_multiple = state.cashout_offer / state.original_stake

        if profit_multiple > 2.0:  # 2x Stake
            # Cashout 50% um Stake zur√ºckzubekommen
            partial_amount = state.original_stake
            if partial_amount < state.cashout_offer:
                return (
                    False,  # Kein Full Cashout
                    f"Partial Cashout Recommended: Lock in stake (‚Ç¨{partial_amount:.2f})",
                    partial_amount
                )

        # DEFAULT: HOLD
        return (
            False,
            f"Hold: EV(Hold)=‚Ç¨{ev_hold:.2f} > EV(Cashout)=‚Ç¨{ev_cashout:.2f} | Confidence={confidence:.2%}",
            0.0
        )

    def calculate_partial_cashout(
        self,
        state: BetState,
        target_risk_reduction: float = 0.50
    ) -> float:
        """
        Berechne optimalen Partial Cashout Betrag

        Args:
            target_risk_reduction: Wieviel % des Risikos absichern (0-1)

        Returns:
            Cashout Amount in Euro
        """
        # Stelle sicher dass wir im Profit sind
        if state.cashout_offer <= state.original_stake:
            return 0.0

        # Berechne Betrag um Stake abzusichern
        amount_to_secure = state.original_stake * target_risk_reduction

        # Pr√ºfe ob m√∂glich
        if amount_to_secure > state.cashout_offer:
            return 0.0  # Nicht genug Profit

        return amount_to_secure


# ==========================================================
# DEEP Q-NETWORK (Advanced)
# ==========================================================

if TORCH_AVAILABLE:
    class CashoutDQN(nn.Module):
        """
        Deep Q-Network f√ºr Cashout-Entscheidungen

        State Space: 12 Features
        - Original Stake
        - Original Odds
        - Current Time
        - Home Score
        - Away Score
        - Home Win Prob
        - Draw Prob
        - Away Win Prob
        - Cashout Offer
        - Peak Cashout
        - xG Momentum
        - Selection (one-hot: 3 dims)

        Action Space: 4 Actions
        - No Action (hold)
        - Cashout 25%
        - Cashout 50%
        - Cashout 100%
        """

        def __init__(self, state_size: int = 15, action_size: int = 4):
            super().__init__()

            self.network = nn.Sequential(
                nn.Linear(state_size, 128),
                nn.ReLU(),
                nn.Dropout(0.2),

                nn.Linear(128, 64),
                nn.ReLU(),
                nn.Dropout(0.2),

                nn.Linear(64, 32),
                nn.ReLU(),

                nn.Linear(32, action_size)
            )

        def forward(self, x):
            return self.network(x)

    class DQNCashoutAgent:
        """
        Deep Q-Learning Agent f√ºr Cashout-Optimierung
        """

        def __init__(
            self,
            state_size: int = 15,
            action_size: int = 4,
            learning_rate: float = 0.001
        ):
            self.state_size = state_size
            self.action_size = action_size

            self.model = CashoutDQN(state_size, action_size)
            self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)
            self.criterion = nn.MSELoss()

            # Hyperparameters
            self.gamma = 0.95  # Discount factor
            self.epsilon = 1.0  # Exploration rate
            self.epsilon_decay = 0.995
            self.epsilon_min = 0.01

        def state_to_tensor(self, state: BetState) -> torch.Tensor:
            """Konvertiere BetState zu Tensor"""
            features = [
                state.original_stake / 100.0,  # Normalize
                state.original_odds / 10.0,
                state.current_time / 90.0,
                state.home_score / 5.0,
                state.away_score / 5.0,
                state.current_home_win_prob,
                state.current_draw_prob,
                state.current_away_win_prob,
                state.cashout_offer / (state.original_stake * state.original_odds),
                state.peak_cashout / (state.original_stake * state.original_odds),
                (state.home_xg_live - state.home_xg) / 3.0,  # xG Momentum
                (state.away_xg_live - state.away_xg) / 3.0,
                # One-hot encode selection
                1.0 if state.selection == 'Home' else 0.0,
                1.0 if state.selection == 'Draw' else 0.0,
                1.0 if state.selection == 'Away' else 0.0,
            ]

            return torch.FloatTensor(features)

        def act(self, state: BetState, training: bool = False) -> int:
            """
            W√§hle Action basierend auf State

            Returns:
                0 = No Action
                1 = Cashout 25%
                2 = Cashout 50%
                3 = Cashout 100%
            """
            # Exploration
            if training and np.random.random() < self.epsilon:
                return np.random.randint(0, self.action_size)

            # Exploitation
            state_tensor = self.state_to_tensor(state).unsqueeze(0)

            self.model.eval()
            with torch.no_grad():
                q_values = self.model(state_tensor)

            return q_values.argmax().item()

        def train_step(
            self,
            state: BetState,
            action: int,
            reward: float,
            next_state: BetState,
            done: bool
        ):
            """Ein Trainings-Schritt"""
            state_tensor = self.state_to_tensor(state).unsqueeze(0)
            next_state_tensor = self.state_to_tensor(next_state).unsqueeze(0)

            # Current Q
            self.model.train()
            current_q = self.model(state_tensor)[0, action]

            # Target Q
            with torch.no_grad():
                next_q = self.model(next_state_tensor).max()

            if done:
                target_q = reward
            else:
                target_q = reward + self.gamma * next_q

            # Loss
            loss = self.criterion(current_q, torch.tensor(target_q))

            # Optimize
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()

            # Decay epsilon
            if self.epsilon > self.epsilon_min:
                self.epsilon *= self.epsilon_decay

            return loss.item()


# ==========================================================
# CASHOUT SIMULATOR
# ==========================================================
class CashoutSimulator:
    """
    Simuliert Cashout-Szenarien auf historischen Daten

    Verwendet f√ºr:
    - Backtesting von Cashout-Strategien
    - Training von DQN Agent
    """

    def __init__(self, historical_matches: pd.DataFrame):
        self.matches = historical_matches

    def simulate_live_probabilities(
        self,
        minute: int,
        home_score: int,
        away_score: int,
        final_home: int,
        final_away: int
    ) -> Tuple[float, float, float]:
        """
        Simuliere Live-Wahrscheinlichkeiten basierend auf Spielstand + Zeit

        Simple Heuristik (kann mit echten Live-Daten ersetzt werden)
        """
        time_remaining = max(0, 90 - minute)

        # Basis-Wahrscheinlichkeiten basierend auf aktuellem Stand
        score_diff = home_score - away_score

        if score_diff > 0:  # Home f√ºhrt
            home_prob = 0.60 + (score_diff * 0.10)
            draw_prob = 0.25
            away_prob = 0.15
        elif score_diff < 0:  # Away f√ºhrt
            home_prob = 0.15
            draw_prob = 0.25
            away_prob = 0.60 + (abs(score_diff) * 0.10)
        else:  # Unentschieden
            home_prob = 0.35
            draw_prob = 0.30
            away_prob = 0.35

        # Zeit-Anpassung (weniger Zeit = stabiler)
        time_factor = time_remaining / 90.0

        # Bei wenig Zeit wird f√ºhrendes Team wahrscheinlicher
        if minute > 75:
            if score_diff > 0:
                home_prob += 0.15 * (1 - time_factor)
            elif score_diff < 0:
                away_prob += 0.15 * (1 - time_factor)

        # Normalisierung
        total = home_prob + draw_prob + away_prob
        home_prob /= total
        draw_prob /= total
        away_prob /= total

        return home_prob, draw_prob, away_prob


# ==========================================================
# EXAMPLE USAGE
# ==========================================================
def example_cashout_decision():
    """Beispiel: Cashout-Entscheidung"""

    print("üí∞ CASHOUT OPTIMIZER - EXAMPLE")
    print("="*60)

    # Beispiel-Scenario
    state = BetState(
        original_stake=100.0,
        original_odds=2.50,
        selection='Home',
        current_time=65,  # 65. Minute
        home_score=1,
        away_score=0,
        current_home_win_prob=0.72,
        current_draw_prob=0.18,
        current_away_win_prob=0.10,
        cashout_offer=190.0,  # ‚Ç¨190 Cashout-Angebot
        peak_cashout=195.0  # Peak war ‚Ç¨195
    )

    print(f"\nüìä Situation:")
    print(f"  Wette: {state.selection} Win @ {state.original_odds}")
    print(f"  Stake: ‚Ç¨{state.original_stake}")
    print(f"  Potentieller Gewinn: ‚Ç¨{state.original_stake * state.original_odds:.2f}")
    print(f"\n‚öΩ Match Status:")
    print(f"  Zeit: {state.current_time}'")
    print(f"  Stand: {state.home_score}-{state.away_score}")
    print(f"  Aktuelle Wahrscheinlichkeit (Home Win): {state.current_home_win_prob:.2%}")
    print(f"\nüíµ Cashout:")
    print(f"  Angebot: ‚Ç¨{state.cashout_offer:.2f}")
    print(f"  Peak: ‚Ç¨{state.peak_cashout:.2f}")

    # Heuristic Optimizer
    optimizer = HeuristicCashoutOptimizer()

    should_cashout, reason, amount = optimizer.should_cashout(state)

    print(f"\nü§ñ ENTSCHEIDUNG:")
    print(f"  {'‚úÖ CASHOUT' if should_cashout else '‚è∏Ô∏è  HOLD'}")
    print(f"  Grund: {reason}")

    if amount > 0 and not should_cashout:
        print(f"  üí° Empfehlung: Partial Cashout ‚Ç¨{amount:.2f}")

    # Expected Values
    ev_hold = optimizer.calculate_expected_value(state)
    print(f"\nüìä Expected Values:")
    print(f"  EV (Hold): ‚Ç¨{ev_hold:.2f}")
    print(f"  EV (Cashout): ‚Ç¨{state.cashout_offer:.2f}")
    print(f"  Differenz: ‚Ç¨{abs(ev_hold - state.cashout_offer):.2f}")

    # Confidence
    confidence = optimizer.calculate_confidence(state)
    print(f"\nüéØ Confidence Score: {confidence:.2%}")

    print(f"\n{'='*60}\n")


if __name__ == "__main__":
    example_cashout_decision()

    if TORCH_AVAILABLE:
        print("\nüß† Deep Q-Network Available!")
        print("Training requires historical live-betting data.")
    else:
        print("\n‚ö†Ô∏è  PyTorch not installed. Deep Q-Learning unavailable.")
        print("Install: pip install torch")
